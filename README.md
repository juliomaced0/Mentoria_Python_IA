# Mentoria_Python_IA
Repósitorio com as atividades realizadas durante a mentoria de Python para Inteligência Artificial, oferecida pela Dell Lead.
## Atividade 1
Revisão de alguns conceitos básicos de Python, que são frequêntemente utilizados em IA.
## Atividade 2
Exercícios sobre o uso das bibliotecas Pandas, Numpy e Matplotlib para analise de dados.
## Atividade 3
- Parte 1: Treinamento de um modelo de Regressão Linear utilizando a biblioteca Scikit-Learn para análise do dataset 'tips.csv'. Foi realizado o cálculo do erro quadrático médio do modelo, juntamente da obtenção de seus pesos e de uma predição com um valor determinado na atividade. Por último foi criada uma visualização utilizando a biblioteca Matplotlib.
- Parte 2: Criação de três modelos de Regressão Linear, utilizando diferentes atributos do dataset 'california_housing_train.csv'. Os atributos foram escolhidos perante uma análise das correlações que apresentavam entre si, no fim da atividade foi realizada uma comparação entre os erros quadráticos médios de cada modelo, então, foi feita a escolha do melhor entre eles.
- Parte 3: Na parte 3 também foi utilizado o dataset 'california_housing_train.csv', desta vez foram criados três modelos de Regressão Linear utilizando transformações não-lineares dos atributos(x^2, x^3, x^4...), e então foi feito um plot para cada modelo das curvas de regressão comparadas com os Scatterplots. Por fim, foi realizado o cálculo dos MSE e a análise de qual modelo se adaptou melhor aos dados.
- Parte 4: Na parte 4 foi utilizado o mesmo dataset das partes 2 e 3, nesta etapa houve uma introdução ao método da validação cruzada(Conjuntos de treino, validação e teste), juntamente com tecnicas de engenharia de atributos(Seleção de atributos, Atributos polinomiais, Normalização) e seleção de hiperparâmetros(Passo de aprendizagem, Coeficiente de regularização, Número de iterações). Foram treinados 5 modelos lineares utilizando o SGDRegressor do Scikit-Learn e em dois deles foi utilizado o método GridSearch para realizar a otimização de hiperparâmetros. Por fim foi criada uma tabela com os MSE de cada modelo, com o intuito de possibilitar uma melhor comparação, e então o melhor modelo foi escolhido para ser treinado no conjunto de teste.
## Atividade 4
Introdução a problemas de classificação, utilizando modelos de Regressão Logística no dataset 'load_breast_cancer', também foram apresentados conceitos sobre a importância do balanceamentos das classes e novas métricas(Acurácia, Precisão, Recall, F1 Score e Curva ROC). Foram criados quatro modelos que foram comparados em todas as métricas estudadas, e um desses modelos fez o uso do algoritmo SVM, que até então não havia sido estudado na mentoria.
## Atividade 5
Analise de um problema de classificação de dígitos em imagens utilizando o dataset 'MNIST'. Nesta atividade foi feita uma introdução ao uso de MLP, e também foi feito um breve experimento a parte, onde plotei dois gráficos, o primeiro era um gráfico da quantidade de neurônios da MLP em relação a sua acurácia, e o segundo gráfico comparava o aumento no tempo de treinamento da MLP com diferentes quantidades de neurônios. Também foi apresentado um novo método de otimização de hiperparâmetros, o RandomSearch.
## Atividade 6
Apresentação de um algoritmo de Machine Learning que não havia sido visto durante a mentoria. Durante a apresentação, foi utilizado um slide que estará na pasta referente a esta atividade, e foi feita uma breve explicação sobre a teoria por trás do algoritmo RandomForest, seguida de uma desmonstração prática deste algoritmo, e uma breve comparação de sua performance com a de outro algoritmo de Machine Learning, além da plotagem de um gráfico comparando o tempo que cada algoritmo gastou em cada etapa.
## Atividade 7
Introdução a técnicas de aprendizado não-supervisionado, como Clusterização e Redução de dimensionalidade. Na primeira parte, foi aplicada a técnica de Clusterização no dataset 'Mall_Customers.csv', e houve a utilização do método do cotovelo para escolher o número ideal de Clusters. Por fim, cada Cluster foi analisado com base em seus centroides para entender o tipo de cliente que ele representava. Na segunda parte, foi aplicada uma técnica de redução de dimensionalidade utilizando o algoritmo PCA, e feita uma breve comparação entre dois modelos semelhantes, sendo que a redução de dimensionalidade foi aplicada em apenas um deles, com o intuito de possibilitar uma melhor compreensão da melhoria obtida ao utilizar a redução de dimensionalidade.
## Atividade 8
Introdução ao processamento de linguagem natural(NLP), utilizando o dataset '20newsgroups'. Nesta atividade fez-se necessária a criação de diferentes funções com o intuito de realizar um breve pré-processamento(remoção de pontuação, stopwords, números, etc...) no texto que seria analisado, e então foram utilizados e comparados os dois métodos de vetorização de texto mais conhecidos atualmente, o Bag-of-Words e o TF-IDF. Durante a atividade, foi feita uma comparação destes métodos com e sem o uso de pré-processamento, para mostrar o ganho obtido nos resultados ao aplicar as funções.
## Trabalho final
No trabalho final, foram utilizadas todas as técnicas estudadas anteriormente para resolver um problema de detecção de sarcasmo, utilizando o dataset 'SARC'. Durante a análise exploratória dos dados foram criados 6 gráficos com informações que foram consideradas importantes durante o estudo do dataset, e também foi aplicada uma técnica de aprendizado não-supervisionado que não foi vista durante a mentoria, chamada de 'Topic Modelling'. Foram criados dois conjuntos de atributos, e cada conjunto foi treinado em 4 algoritmos escolhidos por mim, sendo um deles o PassiveAgressiveClassifier, que não foi estudado durante a mentoria. Por fim, comparei todos os resultados obtidos em uma tabela e escolhi o meu modelo que apresentou o melhor resultado para construir uma matriz de confusão, que possibilitou uma análise da quantidade de erros cometidos naquele modelo, após isso, foi realizada uma breve conclusão explicando os resultados obtidos. Durante cada etapa desta atividade foi exigida uma breve explicação do que estava sendo feito, das minhas escolhas, e dos resultados obtidos, essas explicações mais detalhadas podem ser vistas no notebook referente a esta atividade.
