{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24d55a8",
   "metadata": {},
   "source": [
    "# Atividade 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab59b8",
   "metadata": {},
   "source": [
    "Nessa atividade vocês irão trabalhar em um problema de classificação de texto multiclasse. Considere o conjunto de dados sobre fetch_20newsgroups  \n",
    "\n",
    "    \"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "    \n",
    "Dado esse contexto, escolha um único classificador, sem otimizar hiperparametros, treine e teste modelos considerando\n",
    "\n",
    "    Bag of Words (contagem), sem pré-processamento\n",
    "    TF-IDF, sem pré-processamento \n",
    "    Bag of Words, com pré-processamento\n",
    "    TF-IDF, com pré-processamento\n",
    "    \n",
    "Considere a métrica da acurácia e compare os resultados em uma tabela.\n",
    "\n",
    "As etapas de pré-processamento devem conter pelo menos:\n",
    "\n",
    "    lowercase\n",
    "    remoção de pontuação\n",
    "    remoção de números \n",
    "    remoção de stopwords (dica: utilize a biblioteca NLTK)\n",
    "    lematização ou stemming (apenas um dos dois)\n",
    "    \n",
    "Outras etapas que você julgar necessárias podem ser utilizadas. Crie uma função para cada etapa e uma função chamada preprocess() que chame todas as etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "247ac691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "95e1f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "029425b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bf093c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = twenty_train.data\n",
    "y_train = twenty_train.target\n",
    "data_test = twenty_test.data\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "72ec0b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "42a2ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_pontuacao(text):\n",
    "    sem_pontuacao=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return sem_pontuacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "762dbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "48b78f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def remove_stop_words(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    texto = [w for w in text.split() if w not in stopwords]\n",
    "    frase = \" \".join(texto)\n",
    "    return frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8c4b6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_nums(text):\n",
    "    num_regex = '\\d+'\n",
    "    t = re.sub(num_regex, '', text)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1ff066c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    texto = [wordnet_lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    frase = \" \".join(texto)\n",
    "    return frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "544e1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "        N = remove_nums(text)\n",
    "        L = lower(N)\n",
    "        P = remover_pontuacao(L)\n",
    "        R = remove_stop_words(P)\n",
    "        lem = lemmatizer(R)\n",
    "        return lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b8e95808",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(len(data_train)):\n",
    "    F = preprocess(data_train[i])\n",
    "    X_train.append(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0be694de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(len(data_test)):\n",
    "    F = preprocess(data_test[i])\n",
    "    X_test.append(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "200596b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a5386f11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lerxstwamumdedu wheres thing subject car nntppostinghost racwamumdedu organization university maryland college park line wondering anyone could enlighten car saw day door sport car looked late early called bricklin door really small addition front bumper separate rest body know anyone tellme model name engine spec year production car made history whatever info funky looking car please email thanks il brought neighborhood lerxst'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2efcbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "bw = CountVectorizer()\n",
    "X_train_bw = bw.fit_transform(X_train)\n",
    "X_test_bw = bw.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "962b8020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "N = KNeighborsClassifier()\n",
    "N.fit(X_train_bw, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fed7f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = N.predict(X_test_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2328032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A acurácia foi de: 29.86%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "A = accuracy_score(y_test, y_predict)\n",
    "print(f'A acurácia foi de: {round(A*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b38b55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
